---
marp: true
paginate: true
title: Week 11 — Boot Architecture and Memory Management
author: Nitin Chandrachoodan
theme: gaia
style: |
  .columns-2 { display: grid; grid-template-columns: repeat(2, 1fr); gap: 1rem; font-size: 0.8em; }
  .columns-3 {
    display: grid;
    grid-template-columns: repeat(3, minmax(0, 1fr));
    gap: 1rem;
    font-size: 0.65em;
  }
  code { font-size: 0.85em; }
math: mathjax
---

<!-- _class: lead -->

# Boot Architecture and Memory Management

Complete Boot Flow • Memory Architecture • Build Systems

---

# Goals

- Understand complete boot sequence from power-on to userspace
- Learn FPGA bitstream loading strategies
- Explain virtual vs physical memory and MMU role
- Understand reserved memory regions for DMA
- Overview of cross-compilation and build systems

---

### Boot Sequence: Power On to Userspace

![height:4cm](../assets/boot-flow.svg)

1. ROM Code: immutable, loads SPL from SD/flash
2. SPL: minimal bootloader, DRAM init, load U-Boot
3. U-Boot: load kernel/DTB/initramfs, pass boot args
4. Kernel: decompress, parse DT, probe drivers
5. Init: start userspace (busybox/systemd)

---

# ROM Code and SPL (Generic ARM)

<div class="columns-2">
<div>

**ROM Code**
- in SoC, immutable
- First code executed at power-on
- Initialize clocks, minimal I/O
- Load Secondary Program Loader (SPL) from boot media
- **very limited capacity**

</div>
<div>

**SPL**
- small, fits in on-chip SRAM
- Initialize DRAM controller
- Load full U-Boot from storage
- Transfer control to U-Boot
- **under user control, but still limited capacity**

</div>
</div>

Why two stages? ROM limited; DRAM needed for full bootloader

---

# Zynq-Specific Boot Flow

![height:11cm](../assets/zynq-boot-comparison.svg)

---

### FSBL (First Stage Boot Loader)

Xilinx-specific bootloader **generated by Vitis** from HW design

<div class="columns-2">
<div>

**Key Responsibilities**
- PS Config (clocks, MIOs, peripherals)
- DDR controller initialization
- Optional FPGA bitstream loading
- Handoff to U-Boot (SSBL)

</div>
<div>

**Key Differences**
- Hardware-design-aware
- Generated from Vivado project
- Contains board-specific init
- Can program FPGA before U-Boot

</div>
</div>

**BOOT.bin**: BootROM header + FSBL + (optional bitstream) + U-Boot

---

# U-Boot Bootloader

<div class="columns-2">
<div>

![height:13cm](../assets/memory-layout-kernel-entry.svg)

</div>

<div>

Key responsibilities:
- Load kernel, DTB, initramfs from storage
- Pass boot arguments to kernel (command line, DTB)
- Set up memory layout, jump to kernel entry
- (Hardware already initialized by FSBL)

Components loaded at non-overlapping physical addresses
</div>
</div>


---

# FPGA Bitstream Loading

![height:12cm](../assets/bitstream-loading-timeline.svg)

---

# Very Early Boot (FSBL)

FSBL includes bitstream in BOOT.bin, programs FPGA first

<div class="columns-2">
<div>

**Pros**
- Earliest possible configuration
- FPGA ready for all boot stages
- Single BOOT.bin file
- Simplest for production

</div>
<div>

**Cons**
- Largest BOOT.bin size
- Must regenerate BOOT.bin to change
- Most inflexible
- Longer boot time

</div>
</div>

Use case: Fixed production configuration, single bitstream

---

# Early Boot Loading (U-Boot)

U-Boot programs FPGA before launching kernel (bitstream separate from BOOT.bin)

<div class="columns-2">
<div>

**Pros**
- Hardware ready at kernel boot
- Drivers assume FPGA configured
- Simpler initialization
- Single configuration step

</div>
<div>

**Cons**
- Static configuration
- Changing bitstream requires reboot
- Single bitstream per boot
- Larger boot partition

</div>
</div>

Use case: Fixed hardware configuration, production systems

---

# Runtime Loading (FPGA Manager)

Kernel driver manages FPGA configuration after boot

<div class="columns-2">
<div>

**Pros**
- Dynamic reconfiguration
- Switch bitstreams without reboot
- Partial reconfiguration support
- On-demand loading

</div>
<div>

**Cons**
- More complex driver design
- Late binding complexity
- Requires FPGA manager driver
- Delayed hardware availability

</div>
</div>

Use case: Flexible systems, development, multi-configuration

---

### Kernel Initialization

![height:16cm](../assets/kernel-boot-flow.svg)

---

# Device Tree in Boot Flow

Communication mechanism between bootloader and kernel

1. U-Boot loads DTB into RAM at known address
2. U-Boot passes DTB address to kernel (ARM r2 register)
3. Kernel parses DTB early (before full MMU init)
4. DT used for driver binding, resource allocation
5. Runtime access via `/proc/device-tree`

Benefit: Same kernel binary boots on different hardware

---

# Init System

Kernel executes `/sbin/init` to start userspace

<div class="columns-2">
<div>

**Busybox init**
- Simple, minimal footprint
- Common in embedded systems
- Single-threaded startup
- Lightweight

</div>
<div>

**systemd**
- Full-featured
- Parallelized service management
- From desktop/server world
- More complex

</div>
</div>

Choice depends on system complexity and requirements

---

<!-- _class: lead -->

# Part 2: Memory Architecture

Virtual Memory • MMU • Reserved Regions

---

# Why Virtual Memory?

![height:13cm](../assets/virtual-physical-memory.svg)
- **Isolation**: Each process sees independent 0–4GB space
- **Protection**: Kernel enforces R/W/X permissions
- **Relocation**: Physical memory fragmented; virtual contiguous
- **Demand paging**: Load only needed pages into RAM

Example: Two processes both access 0x1000, MMU maps to different physical frames

---

### MMU and Address Translation

![height:14cm](../assets/page-table-walk.svg)

---

# ARM Page Tables (32-bit)

Two-level page table structure:

```
Virtual Address (32-bit):
  [31:20] -> Level 1 index (4096 entries, 1MB sections)
  [19:12] -> Level 2 index (256 entries, 4KB pages)
  [11:0]  -> Offset within page
```

Page table entry contains:
- Physical frame number (20 bits)
- Permission flags (R/W/X, user/kernel)
- Caching attributes

---

### Kernel vs User Address Space

![height:14cm](../assets/address-space-split.svg)
- 0x00000000–0xBFFFFFFF: User (3GB, per-process)
- 0xC0000000–0xFFFFFFFF: Kernel (1GB, shared)

Kernel space:
- Kernel code/data (direct-mapped)
- MMIO regions (via `ioremap`)
- vmalloc area (non-contiguous allocations)

---

# ioremap and MMIO

```c
// Driver maps peripheral registers
void __iomem *base = ioremap(0x43C00000, 0x1000);
```

`ioremap()` creates kernel virtual mapping to physical MMIO region

MMU translates driver's virtual address to peripheral physical address

Allows uniform memory access model for I/O

---

# Reserved Memory Regions

![height:14cm](../assets/physical-memory-layout.svg)

Solution: Reserve memory at boot before fragmentation

Device tree `reserved-memory` node

---

### Reserved Memory in Device Tree

```dts
reserved-memory {
    #address-cells = <1>;
    #size-cells = <1>;
    ranges;

    cma_reserved: cma@20000000 {
        compatible = "shared-dma-pool";
        reg = <0x20000000 0x4000000>;  // 64MB
        reusable;
        linux,cma-default;
    };

    fpga_buffer: fpga@24000000 {
        compatible = "shared-dma-pool";
        reg = <0x24000000 0x1000000>;  // 16MB
        no-map;  // Don't map in kernel linear region
    };
};
```

---

### CMA: Contiguous Memory Allocator

**Problem**: DMA needs contiguous buffers; fragmentation causes failures

**CMA Solution**:
1. Reserve large region at boot (e.g., 64MB)
2. Kernel uses it for movable pages (not pinned)
3. DMA allocation: migrate pages out, allocate contiguous block
4. After DMA: return to CMA pool

Allows large contiguous allocations even on fragmented system

---

# CMA Usage

Driver perspective:

```c
// Allocates from CMA pool if needed
buf = dma_alloc_coherent(&pdev->dev, 1024*1024, &dma_handle, GFP_KERNEL);
```

Kernel automatically uses CMA when:
- Allocation is large
- Physically contiguous required
- CMA pool configured

---

# IOMMU: DMA Protection

![height:14cm](../assets/dma-iommu-comparison.svg)

**IOMMU Solution**: MMU for DMA devices

---

# IOMMU Concept

DMA devices use "DMA virtual addresses" (IOVA)

IOMMU translates IOVA $\rightarrow$ physical (like MMU for CPU)

Kernel maps only valid buffers; out-of-bounds DMA faults

Benefits:
- Security: isolate device access
- Safety: catch DMA bugs
- Virtualization: guest OS DMA isolation

Driver uses same API; IOMMU transparent

---

# IOMMU in Device Tree

```dts
smmu: iommu@40000000 {
    compatible = "arm,smmu-v2";
    reg = <0x40000000 0x10000>;
    #iommu-cells = <1>;
};

fir_filter: fir@43c00000 {
    compatible = "acme,fir-v1";
    reg = <0x43c00000 0x10000>;
    iommus = <&smmu 0>;  // Use IOMMU protection
};
```

---

# Cache Hierarchy

![height:14cm](../assets/cache-hierarchy.svg)
- L1: per-core, 32KB I + 32KB D
- L2: shared, 512KB–1MB
- Memory: DDR, GBs

Coherency protocols (MESI) keep CPU caches consistent

DMA complicates coherency 

---

# Cache Coherency Solutions

<div class="columns-2">
<div>

**Coherent DMA**
- Hardware snoops caches
- Automatic synchronization
- Simpler driver code
- Slower performance

</div>
<div>

**Streaming DMA**
- Explicit sync with `dma_sync_*()`
- Manual cache management
- More complex driver
- Better performance

</div>
</div>

Also: **Uncached mappings** (simple, lowest performance)

For high-rate FPGA peripherals: prefer streaming DMA

---

# Memory Summary

Virtual memory provides isolation via MMU translation

Kernel and user address spaces separated (security)

Reserved memory ensures contiguous buffers for DMA

CMA manages contiguous allocations dynamically

IOMMU adds security layer for DMA transactions

Cache coherency critical for DMA correctness

---

<!-- _class: lead -->

# Part 3: Build Systems

Cross-Compilation • Kernel Config • Yocto/Buildroot

---

# Cross-Compilation

![height:13cm](../assets/cross-compilation.svg)

Cross-toolchain required:
- Cross-compiler: `arm-linux-gnueabihf-gcc`
- Cross-binutils: assembler, linker for ARM
- Target C library: glibc/musl for ARM
- Sysroot: headers and libraries

Output: ARM binaries from x86 build

---

# Kernel Configuration

Thousands of config options control features

<div class="columns-2">
<div>

Key decisions:
- **Modules vs built-in**: Size vs flexibility vs boot time
- **Size optimization**: Disable unused features
- **Hardware support**: Platform-specific drivers

</div>
<div>

Essential for FPGA + Linux:
- Device tree support (CONFIG_OF)
- Platform devices, CMA, GIC
- FPGA manager framework
</div>
</div>

---

# Build Systems Overview

Automate building complete bootable system

Generate: bootloader, kernel, device tree, rootfs

<div class="columns-2">
<div>

**Buildroot**
- Make-based, simpler
- Fast learning curve
- Good for prototyping
- Generates SD card images

</div>
<div>

**Yocto**
- Layer-based, flexible
- Industry standard
- Steep learning curve
- Better customization

</div>
</div>

Both handle cross-toolchain, dependencies, configuration

---

# Summary

- Boot sequence: ROM $\rightarrow$ SPL $\rightarrow$ U-Boot $\rightarrow$ Kernel $\rightarrow$ Init
- FPGA bitstreams: early (static) vs runtime (dynamic)
- Virtual memory provides isolation and protection via MMU
- Reserved memory (CMA) enables large contiguous DMA buffers
- IOMMU adds security for DMA transactions
- Cross-compilation and build systems automate embedded Linux development
